{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4533dff",
   "metadata": {},
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a0cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (should be alphabetized)\n",
    "\n",
    "# Relative imports first\n",
    "from library.python import OSUtils\n",
    "from library.python import GenomeUtils\n",
    "from library.python import RequestUtils\n",
    "\n",
    "# Globbing (file pattern searching)\n",
    "import glob\n",
    "\n",
    "# File I/O\n",
    "import os\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# For Isom code\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# System calls\n",
    "import subprocess\n",
    "\n",
    "# Batch IDs\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302bc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- !!! IMPORTANT !!! --- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the home directory.\n",
    "home_directory = '/home/aeros/'\n",
    "\n",
    "# Set the library directory.\n",
    "\n",
    "# Note that we did not do this in the\n",
    "# Lab 4 notebook.\n",
    "library_directory = home_directory + 'analyses/git_repos/pib792/lab_5/library/'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23d9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- !!! IMPORTANT !!! --- #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the working directory.\n",
    "\n",
    "# Path must be absolute, cannot use '~/analyses'.\n",
    "os.chdir(home_directory + 'analyses/lab_5/')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed63f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a batch ID.\n",
    "# Note the str(batch_id) typecast.\n",
    "batch_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f396e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to hold the batch folder.\n",
    "batch_folder = os.getcwd() + '/batches/' + batch_id\n",
    "\n",
    "# Create the batch folder and enter it.\n",
    "os.makedirs(batch_folder)\n",
    "os.chdir(batch_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53aa6c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aeros/analyses/lab_5/batches/bb268c21-af06-4394-8cc3-d0a923091139\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're in the right folder.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99cd99f",
   "metadata": {},
   "source": [
    "# Step 1 - Motif search using a smaller consensus motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4cc787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLAST isn't great at short sequences (<= 20 nt).\n",
    "\n",
    "# So, let's use a specialized script for searching\n",
    "# for short sequence matches.\n",
    "\n",
    "# In effect, this means modifying Steps 3 and 4 from\n",
    "# the Lab 4 notebook and using a specialized R script\n",
    "# which searches for exact match short sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e9ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/001_r_small_motif/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e76b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39efa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "GU = GenomeUtils.GenomeUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aed63ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Mosi motif.\n",
    "mosi_motif = 'AGCCTAGCT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3947bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random sequences.\n",
    "sequences = GU.random_motif(\n",
    "        n = 9,\n",
    "        t = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bdebd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to append the Mosi motif to the\n",
    "# random sequences so that we can call the\n",
    "# R script which searches for short matches.\n",
    "sequences.append(mosi_motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52c9fa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: \"Rscript /home/aeros/analyses/git_repos/pib79...>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "NULL\n",
      "\n",
      "[[2]]\n",
      "NULL\n",
      "\n",
      "[[3]]\n",
      "NULL\n",
      "\n",
      "[[4]]\n",
      "NULL\n",
      "\n",
      "[[5]]\n",
      "NULL\n",
      "\n",
      "[[6]]\n",
      "NULL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try the sequences in the short sequence search R script.\n",
    "subprocess.Popen(\"Rscript \" \n",
    "                 + library_directory + 'R/small_motif_search.r -g ' \n",
    "                 + home_directory + 'analyses/genomes/a_thaliana/ensembl/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa -m \\'' + ' '.join(sequences) + '\\' -w ' + output_folder, \n",
    "                shell = True)\n",
    "\n",
    "\n",
    "# Use the sequence as the output file name.\n",
    "# for sequence in fasta_files:\n",
    "#     GU.call_blast(\n",
    "#         db_path = blastdb,\n",
    "#         name = sequence.split('/')[-1],\n",
    "#         sequence = sequence,\n",
    "#         write_to = output_folder\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47995cae",
   "metadata": {},
   "source": [
    "# Step 2 - See what matches between a given sequence and the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b93516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/002_treatment_match/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84690872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebfd402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where the BLAST matches are as well\n",
    "# as the peaks file.\n",
    "\n",
    "# Define the Lab 4 batch that we will pull peaks from.\n",
    "lab_4_batch = '6a164690-a690-405d-8b61-56a4608e1d21'\n",
    "blast_matches = os.getcwd() + '/001_r_small_motif/*.results'\n",
    "peaks = home_directory + 'analyses/lab_4/batches/' + lab_4_batch + '/002_peaks_by_category/*.peaks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4075307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the blast matches and the peaks files.\n",
    "match_list = []\n",
    "\n",
    "for m in glob.glob(blast_matches):\n",
    "    match_list.append(m)\n",
    "\n",
    "peaks_list = []\n",
    "\n",
    "for p in glob.glob(peaks):\n",
    "    peaks_list.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "034a9e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in seq.default(start, stop) : 'to' must be of length 1\n",
      "Calls: [ -> [.data.table -> seq -> seq.default\n",
      "Execution halted\n",
      "Error in seq.default(start, stop) : 'to' must be of length 1\n",
      "Calls: [ -> [.data.table -> seq -> seq.default\n",
      "Execution halted\n",
      "Error in seq.default(start, stop) : 'to' must be of length 1\n",
      "Calls: [ -> [.data.table -> seq -> seq.default\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "# Go over each BLAST matches file and compare\n",
    "# to each peaks file.\n",
    "\n",
    "# Note that the message \"Error in seq.default(start, stop) : 'from' must be of length 1\"\n",
    "# indicates that BLAST couldn't find any matches for\n",
    "# the given sequence m in match_list.\n",
    "for m in match_list:\n",
    "    for p in peaks_list:\n",
    "        subprocess.Popen(\"Rscript \" + home_directory + \"analyses/git_repos/pib792/lab_5/library/R/blast_to_peaks.r -b \" + m + \" -p \" + p + \" -w \" + output_folder, shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364c70b",
   "metadata": {},
   "source": [
    "# Step 3 - Align the hits to the annotated genome to get gene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdfffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gene names by aligning previous step to modified Ensembl genome.\n",
    "\n",
    "# We can use the annotated genome provided \n",
    "# by Chris to find the specific genes that contained\n",
    "# the motifs searched for.\n",
    "\n",
    "# We already have an R script in the library folder that\n",
    "# can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d24a3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/003_genome_alignment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d817a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "243e7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output folder for this step to use in the next.\n",
    "alignment_folder = output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34b03dca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: 'Rscript /home/aeros/analyses/git_repos/pib79...>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "NULL\n",
      "\n",
      "[[2]]\n",
      "NULL\n",
      "\n",
      "[[3]]\n",
      "NULL\n",
      "\n",
      "[[4]]\n",
      "NULL\n",
      "\n",
      "[[5]]\n",
      "NULL\n",
      "\n",
      "[[6]]\n",
      "NULL\n",
      "\n",
      "[[7]]\n",
      "NULL\n",
      "\n",
      "[[8]]\n",
      "NULL\n",
      "\n",
      "[[9]]\n",
      "NULL\n",
      "\n",
      "[[10]]\n",
      "NULL\n",
      "\n",
      "[[11]]\n",
      "NULL\n",
      "\n",
      "[[12]]\n",
      "NULL\n",
      "\n",
      "[[13]]\n",
      "NULL\n",
      "\n",
      "[[14]]\n",
      "NULL\n",
      "\n",
      "[[15]]\n",
      "NULL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the R script to align to the genome,\n",
    "# using all of the results from the last step.\n",
    "subprocess.Popen(\"Rscript \" + home_directory + \"analyses/git_repos/pib792/lab_5/library/R/genes_from_alignment.r -g \" + home_directory + \"analyses/genomes/a_thaliana/from_chris/from_chris456w0oiatmp\" + \" -l \" + batch_folder + \"/002_treatment_match/ -r peaks -w \" + output_folder, shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1a7cc",
   "metadata": {},
   "source": [
    "# Step 5 - Extract the gene names from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b1ce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use Python to get the gene names directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d65566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/004_extract_gene_names/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd3e8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1212a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files from the last step.\n",
    "genome_match = glob.glob(alignment_folder + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db822d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AGCCATATT\": {\n",
      "        \"common\": [\n",
      "            \"AT1G01490\",\n",
      "            \"AT1G01650\",\n",
      "            \"AT2G01320\",\n",
      "            \"AT4G00030\",\n",
      "            \"AT1G01630\",\n",
      "            \"AT3G01720\",\n",
      "            \"AT2G01110\",\n",
      "            \"AT1G02305\",\n",
      "            \"AT2G01735\",\n",
      "            \"AT3G01790\",\n",
      "            \"AT3G01100\",\n",
      "            \"AT1G02130\",\n",
      "            \"AT1G01090\",\n",
      "            \"AT2G01630\"\n",
      "        ],\n",
      "        \"control\": [\n",
      "            \"AT3G01500\",\n",
      "            \"AT4G00585\",\n",
      "            \"AT1G01470\",\n",
      "            \"AT4G00360\",\n",
      "            \"AT5G01530\"\n",
      "        ],\n",
      "        \"treatment\": [\n",
      "            \"AT5G01750\",\n",
      "            \"AT1G02310\",\n",
      "            \"AT3G01640\",\n",
      "            \"AT3G01930\",\n",
      "            \"AT3G01490\",\n",
      "            \"AT4G00620\",\n",
      "            \"AT1G02205\"\n",
      "        ]\n",
      "    },\n",
      "    \"AGCCTAGCT\": {\n",
      "        \"common\": [\n",
      "            \"AT3G01910\",\n",
      "            \"AT2G01140\",\n",
      "            \"AT5G01470\",\n",
      "            \"AT5G01270\",\n",
      "            \"AT4G00450\",\n",
      "            \"AT1G01910\",\n",
      "            \"AT1G01080\"\n",
      "        ],\n",
      "        \"control\": [],\n",
      "        \"treatment\": [\n",
      "            \"AT3G01480\",\n",
      "            \"AT1G02020\"\n",
      "        ]\n",
      "    },\n",
      "    \"ATTGGATAG\": {\n",
      "        \"common\": [\n",
      "            \"AT1G01490\",\n",
      "            \"AT1G02270\",\n",
      "            \"AT3G01910\",\n",
      "            \"AT3G01340\",\n",
      "            \"AT3G01400\",\n",
      "            \"AT3G01480\",\n",
      "            \"AT3G01090\",\n",
      "            \"AT3G02110\",\n",
      "            \"AT2G01540\",\n",
      "            \"AT2G01480\",\n",
      "            \"AT1G01910\",\n",
      "            \"AT1G01140\",\n",
      "            \"AT3G02070\",\n",
      "            \"AT1G01800\"\n",
      "        ],\n",
      "        \"control\": [\n",
      "            \"AT3G01500\",\n",
      "            \"AT1G02270\",\n",
      "            \"AT2G01250\",\n",
      "            \"AT4G00230\",\n",
      "            \"AT4G00355\"\n",
      "        ],\n",
      "        \"treatment\": [\n",
      "            \"AT3G01400\",\n",
      "            \"AT3G01610\",\n",
      "            \"AT3G01040\",\n",
      "            \"AT3G01640\",\n",
      "            \"AT5G01030\",\n",
      "            \"AT3G01090\",\n",
      "            \"AT5G01450\",\n",
      "            \"AT1G02020\",\n",
      "            \"AT4G00026\",\n",
      "            \"AT1G02130\",\n",
      "            \"AT1G02010\"\n",
      "        ]\n",
      "    },\n",
      "    \"CTTGGTTGC\": {\n",
      "        \"common\": [\n",
      "            \"AT5G01240\",\n",
      "            \"AT5G02120\",\n",
      "            \"AT3G01310\",\n",
      "            \"AT3G02050\",\n",
      "            \"AT5G01980\",\n",
      "            \"AT5G02160\",\n",
      "            \"AT3G01200\",\n",
      "            \"AT1G01170\",\n",
      "            \"AT1G01080\"\n",
      "        ],\n",
      "        \"control\": [\n",
      "            \"AT5G01240\"\n",
      "        ],\n",
      "        \"treatment\": [\n",
      "            \"AT3G01370\",\n",
      "            \"AT5G01340\",\n",
      "            \"AT3G01650\",\n",
      "            \"AT5G02050\",\n",
      "            \"AT5G01980\",\n",
      "            \"AT5G02160\",\n",
      "            \"AT2G01350\",\n",
      "            \"AT3G01060\"\n",
      "        ]\n",
      "    },\n",
      "    \"GTGCGGATT\": {\n",
      "        \"common\": [\n",
      "            \"AT5G01430\",\n",
      "            \"AT2G01100\",\n",
      "            \"AT1G01610\"\n",
      "        ],\n",
      "        \"control\": [\n",
      "            \"AT5G02020\"\n",
      "        ],\n",
      "        \"treatment\": [\n",
      "            \"AT1G01610\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Use some clever string splitting to get the sequence used\n",
    "# and the category of transcriptomic data.\n",
    "seqs_cats = {}\n",
    "\n",
    "# Go over each genome match.\n",
    "for sc in genome_match:\n",
    "    \n",
    "    # Parse the file name to get the sequence and the\n",
    "    # experimental category.\n",
    "    split_up = sc.split('/')[-1].split('_')[0].split('.')\n",
    "    \n",
    "    sequence = split_up[0]\n",
    "    category = split_up[1]\n",
    "    \n",
    "    if sequence not in seqs_cats:\n",
    "        seqs_cats[sequence] = {category: []}\n",
    "    else:\n",
    "        seqs_cats[sequence][category] = []\n",
    "    \n",
    "    # Open this file and pull all gene names.\n",
    "    with open(sc, 'r') as f:\n",
    "        \n",
    "        # Expensive because we are reading the entire line...\n",
    "        \n",
    "        # Skip the header.\n",
    "        next(f)\n",
    "        \n",
    "        # Now go line-by-line and get the gene name.\n",
    "        for line in f.readlines():\n",
    "            seqs_cats[sequence][category].append(line.split('\\t')[3].split('.')[0])\n",
    "        \n",
    "        # Only keep unique gene names.\n",
    "        seqs_cats[sequence][category] = list(set(seqs_cats[sequence][category]))\n",
    "\n",
    "# (Optional) Pretty print\n",
    "import json\n",
    "print(json.dumps(seqs_cats, indent = 4, sort_keys = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa9c2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output.\n",
    "with open(output_folder + 'gene_list.json', 'w') as g:\n",
    "    g.write(json.dumps(seqs_cats, indent = True, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29813a",
   "metadata": {},
   "source": [
    "# Step 6 - Get UniProt identifiers for each gene (all steps optional from here onward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d60130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the UniProt identifier to ultimately get PDB files.\n",
    "\n",
    "# So, let's get the identifiers now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0682175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/map_gene_name_to_uniprot_id/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc5fdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "733ad32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: 122 / 122\n"
     ]
    }
   ],
   "source": [
    "# UniProt gives a good Python example, but it is\n",
    "# quite long and not abstracted.\n",
    "\n",
    "# Normally we would trim this down to just what we needed,\n",
    "# but for purposes of demonstration, we'll use their\n",
    "# code.\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "from xml.etree import ElementTree\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "\n",
    "POLLING_INTERVAL = 3\n",
    "\n",
    "API_URL = \"https://rest.uniprot.org\"\n",
    "\n",
    "\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "\n",
    "def submit_id_mapping(from_db, to_db, ids):\n",
    "    request = requests.post(\n",
    "        f\"{API_URL}/idmapping/run\",\n",
    "        data={\"from\": from_db, \"to\": to_db, \"ids\": \",\".join(ids)},\n",
    "    )\n",
    "    request.raise_for_status()\n",
    "    return request.json()[\"jobId\"]\n",
    "\n",
    "\n",
    "def get_next_link(headers):\n",
    "    re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "\n",
    "def check_id_mapping_results_ready(job_id):\n",
    "    while True:\n",
    "        request = session.get(f\"{API_URL}/idmapping/status/{job_id}\")\n",
    "        request.raise_for_status()\n",
    "        j = request.json()\n",
    "        if \"jobStatus\" in j:\n",
    "            if j[\"jobStatus\"] == \"RUNNING\":\n",
    "                print(f\"Retrying in {POLLING_INTERVAL}s\")\n",
    "                time.sleep(POLLING_INTERVAL)\n",
    "            else:\n",
    "                raise Exception(request[\"jobStatus\"])\n",
    "        else:\n",
    "            return bool(j[\"results\"] or j[\"failedIds\"])\n",
    "\n",
    "\n",
    "def get_batch(batch_response, file_format, compressed):\n",
    "    batch_url = get_next_link(batch_response.headers)\n",
    "    while batch_url:\n",
    "        batch_response = session.get(batch_url)\n",
    "        batch_response.raise_for_status()\n",
    "        yield decode_results(batch_response, file_format, compressed)\n",
    "        batch_url = get_next_link(batch_response.headers)\n",
    "\n",
    "\n",
    "def combine_batches(all_results, batch_results, file_format):\n",
    "    if file_format == \"json\":\n",
    "        for key in (\"results\", \"failedIds\"):\n",
    "            if key in batch_results and batch_results[key]:\n",
    "                all_results[key] += batch_results[key]\n",
    "    elif file_format == \"tsv\":\n",
    "        return all_results + batch_results[1:]\n",
    "    else:\n",
    "        return all_results + batch_results\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_link(job_id):\n",
    "    url = f\"{API_URL}/idmapping/details/{job_id}\"\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    return request.json()[\"redirectURL\"]\n",
    "\n",
    "\n",
    "def decode_results(response, file_format, compressed):\n",
    "    if compressed:\n",
    "        decompressed = zlib.decompress(response.content, 16 + zlib.MAX_WBITS)\n",
    "        if file_format == \"json\":\n",
    "            j = json.loads(decompressed.decode(\"utf-8\"))\n",
    "            return j\n",
    "        elif file_format == \"tsv\":\n",
    "            return [line for line in decompressed.decode(\"utf-8\").split(\"\\n\") if line]\n",
    "        elif file_format == \"xlsx\":\n",
    "            return [decompressed]\n",
    "        elif file_format == \"xml\":\n",
    "            return [decompressed.decode(\"utf-8\")]\n",
    "        else:\n",
    "            return decompressed.decode(\"utf-8\")\n",
    "    elif file_format == \"json\":\n",
    "        return response.json()\n",
    "    elif file_format == \"tsv\":\n",
    "        return [line for line in response.text.split(\"\\n\") if line]\n",
    "    elif file_format == \"xlsx\":\n",
    "        return [response.content]\n",
    "    elif file_format == \"xml\":\n",
    "        return [response.text]\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def get_xml_namespace(element):\n",
    "    m = re.match(r\"\\{(.*)\\}\", element.tag)\n",
    "    return m.groups()[0] if m else \"\"\n",
    "\n",
    "\n",
    "def merge_xml_results(xml_results):\n",
    "    merged_root = ElementTree.fromstring(xml_results[0])\n",
    "    for result in xml_results[1:]:\n",
    "        root = ElementTree.fromstring(result)\n",
    "        for child in root.findall(\"{http://uniprot.org/uniprot}entry\"):\n",
    "            merged_root.insert(-1, child)\n",
    "    ElementTree.register_namespace(\"\", get_xml_namespace(merged_root[0]))\n",
    "    return ElementTree.tostring(merged_root, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "\n",
    "def print_progress_batches(batch_index, size, total):\n",
    "    n_fetched = min((batch_index + 1) * size, total)\n",
    "    print(f\"Fetched: {n_fetched} / {total}\")\n",
    "\n",
    "\n",
    "def get_id_mapping_results_search(url):\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    if \"size\" in query:\n",
    "        size = int(query[\"size\"][0])\n",
    "    else:\n",
    "        size = 500\n",
    "        query[\"size\"] = size\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    parsed = parsed._replace(query=urlencode(query, doseq=True))\n",
    "    url = parsed.geturl()\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    results = decode_results(request, file_format, compressed)\n",
    "    total = int(request.headers[\"x-total-results\"])\n",
    "    print_progress_batches(0, size, total)\n",
    "    for i, batch in enumerate(get_batch(request, file_format, compressed), 1):\n",
    "        results = combine_batches(results, batch, file_format)\n",
    "        print_progress_batches(i, size, total)\n",
    "    if file_format == \"xml\":\n",
    "        return merge_xml_results(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_stream(url):\n",
    "    if \"/stream/\" not in url:\n",
    "        url = url.replace(\"/results/\", \"/stream/\")\n",
    "    request = session.get(url)\n",
    "    request.raise_for_status()\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    return decode_results(request, file_format, compressed)\n",
    "\n",
    "# Get a list of Ensembl Gene IDs.\n",
    "ensembl_gene_ids = []\n",
    "\n",
    "for k, v in seqs_cats.items():\n",
    "    for s, t in v.items():\n",
    "        for i in t:\n",
    "            ensembl_gene_ids.append(i)\n",
    "\n",
    "# Only need unique IDs...\n",
    "ensembl_gene_ids = list(set(ensembl_gene_ids))\n",
    "\n",
    "# Make a simple dictionary with mappings.\n",
    "mappings = {}\n",
    "\n",
    "job_id = submit_id_mapping(\n",
    "    from_db=\"Ensembl_Genomes\", to_db=\"UniProtKB\", ids=ensembl_gene_ids\n",
    ")\n",
    "\n",
    "if check_id_mapping_results_ready(job_id):\n",
    "    link = get_id_mapping_results_link(job_id)\n",
    "    results = get_id_mapping_results_search(link)\n",
    "    \n",
    "    # Equivalently using the stream endpoint which is more demanding\n",
    "    # on the API and so is less stable:\n",
    "    # results = get_id_mapping_results_stream(link)\n",
    "    \n",
    "    # Go over the results and create the mappings.\n",
    "\n",
    "# We only need the UniProt IDs for each result.\n",
    "\n",
    "# Note that we said \"IDs\", because it is possible\n",
    "# to have more than one ID on UniProt.\n",
    "for result in results['results']:\n",
    "    \n",
    "    # See if we have the mapping already.\n",
    "    if result['from'] not in mappings:\n",
    "        mappings[result['from']] = []\n",
    "    \n",
    "    mappings[result['from']].append(result['to']['primaryAccession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0acd9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output.\n",
    "with open(output_folder + 'ensembl_uniprot_mappings.json', 'w') as g:\n",
    "    g.write(json.dumps(seqs_cats, indent = True, sort_keys = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c87819",
   "metadata": {},
   "source": [
    "# Step 7 - Get the isoforms for each Ensembl gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "04464200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The genome that Chris gave was the most liberal in terms\n",
    "# of the gene location, using the most upstream 5'-UTR\n",
    "# and the most downstream 3'-UTR for each gene.\n",
    "\n",
    "# So, we don't actually know specifically where the motif\n",
    "# matches occured in each isoform, which is what this step\n",
    "# is for.\n",
    "\n",
    "# We can use UniProt to get the isoforms.\n",
    "\n",
    "# Try logic out with this mapping, as this gene has more\n",
    "# than one isoform.\n",
    "mappings = {'AT1G16610': ['Q9SEE9-1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4f8a58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output folder for this step.\n",
    "output_folder = os.getcwd() + '/isoforms_from_genes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82903f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder.\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ec57bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go over each Ensembl/UniProt ID mapping.\n",
    "\n",
    "# Code modified from code courtesy of Dan Isom.\n",
    "for ensembl_id, uniprot_ids in mappings.items():\n",
    "    for uniprot_id in uniprot_ids:\n",
    "        \n",
    "        ensgs = {ensembl_id: uniprot_id}\n",
    "        \n",
    "        server = \"https://rest.ensembl.org\"\n",
    "        \n",
    "        keys, sequences = list(ensgs), {}\n",
    "\n",
    "        # Protein sequence\n",
    "        for key in keys:\n",
    "            ext = \"/sequence/id/\" + key + \"?multiple_sequences=1;type=protein\"\n",
    "            r = requests.get(server+ext, headers={ \"Content-Type\" : \"text/plain\"})\n",
    "            if not r.ok:\n",
    "                r.raise_for_status()\n",
    "                sys.exit()\n",
    "            sequence = r.text\n",
    "        \n",
    "            # Split the sequence returned into the various isoforms.\n",
    "            isoforms = sequence.split('\\n')\n",
    "\n",
    "            # Write one file per isoform.\n",
    "            for isoform in range(len(isoforms)):\n",
    "              with open(output_folder + ensembl_id + '-' + uniprot_id + '-' + str(isoform + 1) + '.protein.fa', 'w') as f:\n",
    "                f.write(isoforms[isoform])\n",
    "\n",
    "        # DNA sequence\n",
    "        keys, sequences = list(ensgs), {}\n",
    "\n",
    "        for key in keys:\n",
    "            ext = \"/sequence/id/\" + key + \"?multiple_sequences=1;type=cds\" \n",
    "            r = requests.get(server+ext, headers={ \"Content-Type\" : \"text/plain\"})\n",
    "            if not r.ok:\n",
    "                r.raise_for_status()\n",
    "                sys.exit()\n",
    "            sequence = r.text\n",
    "            \n",
    "            # Split the sequence returned into the various isoforms.\n",
    "            isoforms = sequence.split('\\n')\n",
    "            \n",
    "            # Write one file per isoform.\n",
    "            for isoform in range(len(isoforms)):\n",
    "              with open(output_folder + ensembl_id + '-' + uniprot_id + '-' + str(isoform + 1) + '.dna.fa', 'w') as f:\n",
    "                f.write(isoforms[isoform])\n",
    "\n",
    "        # Wait a small amount to not piss off Ensembl.\n",
    "\n",
    "        # Note: already imported time above in UniProt\n",
    "        # example block.\n",
    "        time.sleep(0.1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e9108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26497b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaFold doesn't have an API available for us\n",
    "# to use to get PDB files.\n",
    "\n",
    "# So, we had to download the entire A. thaliana PDB\n",
    "# database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be3246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "567f8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps\n",
    "\n",
    "# Use UniProt identifier to pull PDB files for given gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvements\n",
    "\n",
    "# Folder names beginning with 001, 002, etc...\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pib792",
   "language": "python",
   "name": "pib792"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
